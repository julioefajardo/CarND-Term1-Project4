{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 10]\n",
    "\n",
    "image_file = 'test_images/test6.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_width = 64 # \n",
    "window_height = 120 #180 con el primer src, 120\n",
    "margin = 40 #40 con el primer src, 60\n",
    "\n",
    "ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "xm_per_pix = 3.7/700 # meters per pixel in x dimension    \n",
    "\n",
    "src = np.float32(\n",
    "    [[0, 720],\n",
    "    [1280, 720],\n",
    "    [510, 480],\n",
    "    [770, 480]])\n",
    "\n",
    "dst = np.float32(\n",
    "    [[0, 720],\n",
    "    [1280, 720],\n",
    "    [0, 0],\n",
    "    [1280, 0]])\n",
    "\n",
    "m = cv2.getPerspectiveTransform(src, dst)\n",
    "m_inv = cv2.getPerspectiveTransform(dst, src)\n",
    "\n",
    "with open('calibration_data/calibration_matrices.p', 'rb') as f:\n",
    "    save_dict = pickle.load(f)\n",
    "mtx = save_dict['mtx']\n",
    "dist = save_dict['dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def abs_sobel_thres(img, orient='x', sobel_thresh=(20, 100)):\n",
    "    \"\"\"\n",
    "    Takes an image, gradient orientation, and threshold min/max values\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Apply x or y gradient with the OpenCV Sobel() function\n",
    "    # and take the absolute value\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1))\n",
    "    # Rescale back to 8 bit integer\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Create a copy and apply the threshold\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    # Here I'm using inclusive (>=, <=) thresholds, but exclusive is ok too\n",
    "    binary_output[(scaled_sobel >= sobel_thresh[0]) & (scaled_sobel <= sobel_thresh[1])] = 1\n",
    "\n",
    "    # Return the result\n",
    "    return binary_output\n",
    "\n",
    "def mag_thres(img, sobel_kernel=3, mag_thresh=(30, 100)):\n",
    "    \"\"\"\n",
    "    Return the magnitude of the gradient\n",
    "    for a given sobel kernel size and threshold values\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Take both Sobel x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Calculate the gradient magnitude\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # Rescale to 8 bit\n",
    "    scale_factor = np.max(gradmag)/255\n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8)\n",
    "    # Create a binary image of ones where threshold is met, zeros otherwise\n",
    "    binary_output = np.zeros_like(gradmag)\n",
    "    binary_output[(gradmag >= mag_thresh[0]) & (gradmag <= mag_thresh[1])] = 1\n",
    "\n",
    "    # Return the binary image\n",
    "    return binary_output\n",
    "\n",
    "def dir_thres(img, sobel_kernel=3, dir_thresh=(0, np.pi/2)):\n",
    "    \"\"\"\n",
    "    Return the direction of the gradient\n",
    "    for a given sobel kernel size and threshold values\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Calculate the x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Take the absolute value of the gradient direction,\n",
    "    # apply a threshold, and create a binary image result\n",
    "    absgraddir = np.arctan2(np.absolute(sobely), np.absolute(sobelx))\n",
    "    binary_output =  np.zeros_like(absgraddir)\n",
    "    binary_output[(absgraddir >= dir_thresh[0]) & (absgraddir <= dir_thresh[1])] = 1\n",
    "\n",
    "    # Return the binary image\n",
    "    return binary_output\n",
    "\n",
    "def HLS_thres(img, HLS_thresh=(100, 255)):\n",
    "    \"\"\"\n",
    "    Convert RGB to HLS and threshold to binary image using S channel\n",
    "    \"\"\"\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    s_channel = hls[:,:,2]\n",
    "    binary_output = np.zeros_like(s_channel)\n",
    "    binary_output[(s_channel > HLS_thresh[0]) & (s_channel <= HLS_thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "def HSV_thres(img, HSV_thresh=(100, 255)):\n",
    "    \"\"\"\n",
    "    Convert RGB to HLS and threshold to binary image using S channel\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    s_channel = hsv[:,:,2]\n",
    "    binary_output = np.zeros_like(s_channel)\n",
    "    binary_output[(s_channel > HSV_thresh[0]) & (s_channel <= HSV_thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "def combined_thresh(img):\n",
    "    abs_bin = abs_sobel_thres(img, orient='x', sobel_thresh=(30, 255))#50\n",
    "    mag_bin = mag_thres(img, sobel_kernel=3, mag_thresh=(50, 255))#60\n",
    "    dir_bin = dir_thres(img, sobel_kernel=15, dir_thresh=(0.7, 1.1))\n",
    "    hls_bin = HLS_thres(img, HLS_thresh=(100, 255))#150\n",
    "    hsv_bin = HSV_thres(img, HSV_thresh=(100, 255))#220\n",
    "    combined = np.zeros_like(dir_bin)\n",
    "    combined[(abs_bin == 1 | ((mag_bin == 1) & (dir_bin == 1))) | ((hls_bin == 1)&(hsv_bin == 1))] = 1\n",
    "\n",
    "    return combined\n",
    "\n",
    "def window_mask(width, height, img_ref, center,level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "    return output\n",
    "\n",
    "def find_window_centroids(warped, window_width, window_height, margin):\n",
    "    \n",
    "    window_centroids = [] # Store the (left,right) window centroid positions per level\n",
    "    window = np.ones(window_width) # Create our window template that we will use for convolutions\n",
    "    \n",
    "    # First find the two starting positions for the left and right lane by using np.sum to get the vertical image slice\n",
    "    # and then np.convolve the vertical image slice with the window template \n",
    "    \n",
    "    # Sum quarter bottom of image to get slice, could use a different ratio\n",
    "    l_sum = np.sum(warped[int(3*warped.shape[0]/4):,:int(warped.shape[1]/2)], axis=0)\n",
    "    l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "    r_sum = np.sum(warped[int(3*warped.shape[0]/4):,int(warped.shape[1]/2):], axis=0)\n",
    "    r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(warped.shape[1]/2)\n",
    "    \n",
    "    # Add what we found for the first layer\n",
    "    window_centroids.append((l_center,r_center))\n",
    "    \n",
    "    # Go through each layer looking for max pixel locations\n",
    "    for level in range(1,(int)(warped.shape[0]/window_height)):\n",
    "        # convolve the window into the vertical slice of the image\n",
    "        image_layer = np.sum(warped[int(warped.shape[0]-(level+1)*window_height):int(warped.shape[0]-level*window_height),:], axis=0)\n",
    "        conv_signal = np.convolve(window, image_layer)\n",
    "        # Find the best left centroid by using past left center as a reference\n",
    "        # Use window_width/2 as offset because convolution signal reference is at right side of window, not center of window\n",
    "        offset = window_width/2\n",
    "        l_min_index = int(max(l_center+offset-margin,0))\n",
    "        l_max_index = int(min(l_center+offset+margin,warped.shape[1]))\n",
    "        l_center = np.argmax(conv_signal[l_min_index:l_max_index])+l_min_index-offset\n",
    "        # Find the best right centroid by using past right center as a reference\n",
    "        r_min_index = int(max(r_center+offset-margin,0))\n",
    "        r_max_index = int(min(r_center+offset+margin,warped.shape[1]))\n",
    "        r_center = np.argmax(conv_signal[r_min_index:r_max_index])+r_min_index-offset\n",
    "        # Add what we found for that layer\n",
    "        window_centroids.append((l_center,r_center))\n",
    "\n",
    "    return window_centroids\n",
    "\n",
    "\n",
    "def window_search(warped,window_centroids):\n",
    "    # If we found any window centers\n",
    "    if len(window_centroids) > 0:\n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(warped)\n",
    "        r_points = np.zeros_like(warped)\n",
    "        # Go through each level and draw the windows \t\n",
    "        for level in range(0,len(window_centroids)):\n",
    "            # Window_mask is a function to draw window areas\n",
    "            l_mask = window_mask(window_width,window_height,warped,window_centroids[level][0],level)\n",
    "            r_mask = window_mask(window_width,window_height,warped,window_centroids[level][1],level)\n",
    "            # Add graphic points from window mask here to total pixels found \n",
    "            l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "            r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "        # Draw the results\n",
    "        template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "        zero_channel = np.zeros_like(template) # create a zero color channle \n",
    "        template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window pixels green\n",
    "        warpage = np.array(cv2.merge((warped,warped,warped)),np.uint8) # making the original road pixels 3 color channels\n",
    "        output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    "    # If no window centers found, just display orginal road image\n",
    "    else:\n",
    "        output = np.array(cv2.merge((warped,warped,warped)),np.uint8)\n",
    "    left_mask = np.concatenate((np.ones((warped.shape[0],int(warped.shape[1]/2),3), dtype=np.uint8)*255, np.zeros((warped.shape[0],int(warped.shape[1]/2),3), dtype=np.uint8)), axis=1)\n",
    "    right_mask = np.concatenate((np.zeros((warped.shape[0],int(warped.shape[1]/2),3), dtype=np.uint8), np.ones((warped.shape[0],int(warped.shape[1]/2),3), dtype=np.uint8)*255), axis=1)\n",
    "    left = cv2.bitwise_and(left_mask,output)\n",
    "    right = cv2.bitwise_and(right_mask,output)\n",
    "    return left,right,template\n",
    "\n",
    "def get_points(image):\n",
    "    points = []\n",
    "    for x in range(0,image.shape[1]):\n",
    "        for y in range(0,image.shape[0]):\n",
    "            if(image[y][x].any()!=0):\n",
    "                points.append([x,y])\n",
    "    return points\n",
    "\n",
    "def curvature(left_points,right_points):\n",
    "    left_fit_cr = np.polyfit(left_points[:,1]*ym_per_pix, left_points[:,0]*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(right_points[:,1]*ym_per_pix, right_points[:,0]*xm_per_pix, 2)\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*719*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*719*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    return left_curverad, right_curverad\n",
    "    \n",
    "\n",
    "def pipeline(image):\n",
    "    image = cv2.undistort(image, mtx, dist, None, mtx)\n",
    "    binary = combined_thresh(image)\n",
    "    warped = cv2.warpPerspective(binary, m, (image.shape[1], image.shape[0]), flags=cv2.INTER_LINEAR)\n",
    "    warped[:,0:201,:] = 0\n",
    "    warped[400:1100,0:701,:] = 0\n",
    "    window_centroids = find_window_centroids(warped, window_width, window_height, margin)\n",
    "    left,right,template = window_search(warped,window_centroids)\n",
    "    left_points = np.transpose(np.asarray(np.nonzero(cv2.cvtColor(left, cv2.COLOR_BGR2GRAY))))\n",
    "    right_points = np.transpose(np.asarray(np.nonzero(cv2.cvtColor(right, cv2.COLOR_BGR2GRAY))))\n",
    "    l_fit = np.polyfit(left_points[:,0],left_points[:,1],2)\n",
    "    r_fit = np.polyfit(right_points[:,0],right_points[:,1],2)\n",
    "    the_points = np.zeros((1,720*2,2), dtype=np.int32)\n",
    "    for i in range(0,720):\n",
    "        the_points[0][i] = [int(l_fit[0]*i*i + l_fit[1]*i + l_fit[2]),i]\n",
    "    for i in range(0,720):\n",
    "        the_points[0][i+720] = [int(r_fit[0]*(719-i)*(719-i) + r_fit[1]*(719-i) + r_fit[2]),(719-i)]\n",
    "    output_pol = np.zeros_like(template)\n",
    "    cv2.fillConvexPoly(output_pol,the_points, (0,255,0))\n",
    "    left_curverad, right_curverad = curvature(left_points,right_points)\n",
    "    center_dev = (image.shape[1]/2-(l_fit[2]+(r_fit[2]-l_fit[2])/2))*xm_per_pix*100\n",
    "    newwarp = cv2.warpPerspective(output_pol, m_inv, (image.shape[1], image.shape[0])) \n",
    "    result = cv2.addWeighted(image, 1, newwarp, 0.3, 0)\n",
    "    cv2.putText(result,'Left Curvature: '+str(left_curverad)+' m',(50,75), cv2.FONT_HERSHEY_SIMPLEX, 1.25,(255,255,255),2)\n",
    "    cv2.putText(result,'Right Curvature: '+str(right_curverad)+' m',(50,125), cv2.FONT_HERSHEY_SIMPLEX, 1.25,(255,255,255),2)\n",
    "    cv2.putText(result,'Lane Deviation: '+str(center_dev)+' cm',(50,175), cv2.FONT_HERSHEY_SIMPLEX, 1.25,(255,255,255),2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c1c5eb9dfc2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-95398309333b>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_thresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mwarped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarpPerspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_LINEAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0mwarped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m201\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0mwarped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m701\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mwindow_centroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_window_centroids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(image_file)\n",
    "result = pipeline(image)\n",
    "result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_output = 'annotated.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "video_clip = clip1.fl_image(pipeline) #NOTE: this function expects color images!!\n",
    "%time video_clip.write_videofile(video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_output = 'challenge_annotated.mp4'\n",
    "clip1 = VideoFileClip(\"challenge_video.mp4\")\n",
    "video_clip = clip1.fl_image(pipeline) #NOTE: this function expects color images!!\n",
    "%time video_clip.write_videofile(video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_output = 'harder_annotated.mp4'\n",
    "clip1 = VideoFileClip(\"harder_challenge_video.mp4\")\n",
    "video_clip = clip1.fl_image(pipeline) #NOTE: this function expects color images!!\n",
    "%time video_clip.write_videofile(video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
